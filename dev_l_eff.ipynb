{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ[\"JAX_PLATFORMS\"] = \"cpu\"\n",
        "\n",
        "from typing import Tuple\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from src.models.base import ModelConfig\n",
        "from src.models.rnn import ElmanRNN, LSTM, UnitaryRNN\n",
        "from src.models.lru import LinearRecurrentUnit\n",
        "from src.data.copy_dataset import CopyDataset\n",
        "\n",
        "Array = jnp.ndarray\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _jacobian_lookback_frobenius(\n",
        "    jac_diags: Array,\n",
        "    wh_weight: Array,\n",
        "    mask: Array,\n",
        ") -> Array:\n",
        "    \"\"\"Compute M(T) = ||J_t * J_{t-1} * ... * J_{t-T+1}||_F for each time step t and lookback T.\n",
        "\n",
        "    For time step t, computes jacobian norms for all possible lookback windows:\n",
        "    - lag=0: identity matrix (norm = sqrt(H))\n",
        "    - lag=1: ||J_t||_F\n",
        "    - lag=2: ||J_t * J_{t-1}||_F\n",
        "    - lag=T: ||J_t * J_{t-1} * ... * J_0||_F\n",
        "\n",
        "    Args:\n",
        "        jac_diags: Jacobian diagonals [B, T, H]\n",
        "        wh_weight: Hidden-to-hidden weight matrix [H, H]\n",
        "        mask: Sequence mask [B, T]\n",
        "\n",
        "    Returns:\n",
        "        Array of shape [B, T, T+1] where output[b, t, lag] = M(lag) at time t\n",
        "    \"\"\"\n",
        "    B, T, H = jac_diags.shape\n",
        "    wh = wh_weight.astype(jac_diags.dtype)\n",
        "    diag_seq = jnp.swapaxes(jac_diags, 0, 1)  # [T, B, H]\n",
        "    mask_seq = jnp.swapaxes(mask, 0, 1)  # [T, B]\n",
        "\n",
        "    # Identity matrix norm (for lag=0)\n",
        "    identity_norm = jnp.sqrt(jnp.array(H, dtype=wh.dtype))\n",
        "    eye = jnp.broadcast_to(jnp.eye(H, dtype=wh.dtype), (B, H, H))\n",
        "\n",
        "    # Initialize output array [B, T, T+1]\n",
        "    all_norms = jnp.zeros((B, T, T + 1), dtype=wh.dtype)\n",
        "\n",
        "    def scan_step(carry, inputs):\n",
        "        \"\"\"Process one time step, computing all lookback norms.\n",
        "        \n",
        "        carry: (jacobian_history [B, T, H, H], norms [B, T, T+1])\n",
        "        inputs: (diag_t, mask_t, t_idx) where t_idx is the current time step\n",
        "        \"\"\"\n",
        "        diag_t, mask_t, t_idx = inputs\n",
        "        jacobian_history, norms = carry\n",
        "        \n",
        "        # Compute J_t for this time step\n",
        "        J_t = diag_t[:, :, None] * wh[None, :, :]  # [B, H, H]\n",
        "        mask_bool = mask_t > 0.0\n",
        "        \n",
        "        # Update jacobian history: shift and prepend J_t\n",
        "        jacobian_history = jnp.concatenate([J_t[:, None, :, :], jacobian_history[:, :-1, :, :]], axis=1)\n",
        "        \n",
        "        # Collect norms for all lags at this time step\n",
        "        time_step_norms = jnp.zeros((B, T + 1), dtype=wh.dtype)\n",
        "        time_step_norms = time_step_norms.at[:, 0].set(identity_norm)  # lag=0: identity\n",
        "        \n",
        "        # Compute norms for lags 1 to t_idx+1 using fori_loop\n",
        "        # We'll compute up to T+1 and mask invalid lags\n",
        "        def compute_lag_norm(lag, carry_state):\n",
        "            J_cumulative, norms_array = carry_state\n",
        "            # Only compute if lag <= t_idx + 1\n",
        "            valid_lag = lag <= (t_idx + 1)\n",
        "            # Get J_{t-lag+1} from history (most recent is at index 0)\n",
        "            hist_idx = lag - 1\n",
        "            J_hist = jacobian_history[:, hist_idx, :, :]  # [B, H, H]\n",
        "            # Multiply cumulative by this jacobian (going backwards in time)\n",
        "            J_cumulative = jnp.einsum(\"bij,bjk->bik\", J_hist, J_cumulative)\n",
        "            # Apply mask\n",
        "            J_cumulative = jnp.where(mask_bool[:, None, None], J_cumulative, eye)\n",
        "            # Compute norm\n",
        "            frob = jnp.linalg.norm(J_cumulative, axis=(-2, -1))\n",
        "            frob = jnp.where(mask_bool, frob, jnp.zeros_like(frob))\n",
        "            # Only update if valid lag\n",
        "            updated_norms = norms_array.at[:, lag].set(frob)\n",
        "            norms_array = jnp.where(valid_lag, updated_norms, norms_array)\n",
        "            return (J_cumulative, norms_array)\n",
        "        \n",
        "        # Start with identity matrix\n",
        "        initial_state = (eye, time_step_norms)\n",
        "        # Compute for lags 1 to T+1 (we'll mask invalid ones)\n",
        "        final_state = jax.lax.fori_loop(1, T + 1, compute_lag_norm, initial_state)\n",
        "        _, time_step_norms = final_state\n",
        "        \n",
        "        # Update norms array\n",
        "        norms = norms.at[:, t_idx, :].set(time_step_norms)\n",
        "        \n",
        "        return (jacobian_history, norms), None\n",
        "\n",
        "    # Initialize carry\n",
        "    jacobian_history = jnp.zeros((B, T, H, H), dtype=wh.dtype)\n",
        "    initial_carry = (jacobian_history, all_norms)\n",
        "    \n",
        "    # Create inputs: (diag_seq, mask_seq, time_indices)\n",
        "    time_indices = jnp.arange(T)\n",
        "    inputs = (diag_seq, mask_seq, time_indices)\n",
        "    \n",
        "    # Scan over time steps\n",
        "    final_carry, _ = jax.lax.scan(scan_step, initial_carry, inputs)\n",
        "    _, final_norms = final_carry\n",
        "    \n",
        "    return final_norms  # [B, T, T+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _compute_l_eff(\n",
        "    lookback_norms: Array,\n",
        "    epsilon_values: Tuple[float, ...],\n",
        "    mask: Array,\n",
        ") -> Array:\n",
        "    \"\"\"Compute l_eff(epsilon) = max{T >= 0 : M(T) > epsilon} for each time step and epsilon.\n",
        "\n",
        "    Args:\n",
        "        lookback_norms: Array of shape [B, T, T+1] where lookback_norms[b, t, lag] = M(lag) at time t\n",
        "        epsilon_values: Tuple of epsilon values to compute l_eff for\n",
        "        mask: Sequence mask [B, T]\n",
        "\n",
        "    Returns:\n",
        "        Array of shape [B, T, num_epsilons] where output[b, t, e] = l_eff(epsilon_values[e]) at time t\n",
        "    \"\"\"\n",
        "    B, T, max_lag = lookback_norms.shape\n",
        "\n",
        "    # Convert epsilon values to array for broadcasting\n",
        "    epsilons = jnp.array(epsilon_values, dtype=lookback_norms.dtype)  # [num_epsilons]\n",
        "\n",
        "    # For each time step t, find maximum lag T where M(T) > epsilon\n",
        "    # lookback_norms[b, t, :] contains M(0), M(1), ..., M(t) (padded with zeros)\n",
        "    # We need to find the maximum lag where the norm > epsilon\n",
        "\n",
        "    # Expand dimensions for broadcasting: [B, T, T+1] vs [num_epsilons]\n",
        "    # We want to compare each norm with each epsilon\n",
        "    lookback_norms_expanded = lookback_norms[:, :, :, None]  # [B, T, T+1, 1]\n",
        "    epsilons_expanded = epsilons[None, None, None, :]  # [1, 1, 1, num_epsilons]\n",
        "\n",
        "    # Compare: [B, T, T+1, num_epsilons]\n",
        "    greater_than_epsilon = lookback_norms_expanded > epsilons_expanded\n",
        "\n",
        "    # For each epsilon, find the maximum lag where condition is true\n",
        "    # Create lag indices: [0, 1, 2, ..., T]\n",
        "    lag_indices = jnp.arange(max_lag, dtype=lookback_norms.dtype)  # [T+1]\n",
        "    lag_indices_expanded = lag_indices[None, None, :, None]  # [1, 1, T+1, 1]\n",
        "\n",
        "    # Where condition is true, use the lag index; where false, use -1\n",
        "    valid_lags = jnp.where(\n",
        "        greater_than_epsilon, lag_indices_expanded, -1.0\n",
        "    )  # [B, T, T+1, num_epsilons]\n",
        "\n",
        "    # Take maximum over lag dimension: [B, T, num_epsilons]\n",
        "    l_eff = jnp.max(valid_lags, axis=2)  # [B, T, num_epsilons]\n",
        "\n",
        "    # If no lag satisfies the condition (all are -1), set to 0\n",
        "    l_eff = jnp.maximum(l_eff, 0.0)\n",
        "\n",
        "    # Apply mask: set l_eff to 0 for masked time steps\n",
        "    mask_expanded = mask[:, :, None]  # [B, T, 1]\n",
        "    l_eff = l_eff * mask_expanded\n",
        "\n",
        "    return l_eff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:2025-12-07 21:03:50,527:jax._src.xla_bridge:473: Jax plugin configuration error: Exception when calling jax_plugins.xla_cuda12.initialize()\n",
            "Traceback (most recent call last):\n",
            "  File \"/home/rphess/conda/envs/recurrent-networks/lib/python3.12/site-packages/jax/_src/xla_bridge.py\", line 471, in discover_pjrt_plugins\n",
            "    plugin_module.initialize()\n",
            "  File \"/home/rphess/conda/envs/recurrent-networks/lib/python3.12/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 328, in initialize\n",
            "    _check_cuda_versions(raise_on_first_error=True)\n",
            "  File \"/home/rphess/conda/envs/recurrent-networks/lib/python3.12/site-packages/jax_plugins/xla_cuda12/__init__.py\", line 285, in _check_cuda_versions\n",
            "    local_device_count = cuda_versions.cuda_device_count()\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: jaxlib/cuda/versions_helpers.cc:113: operation cuInit(0) failed: CUDA_ERROR_NO_DEVICE\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs one-hot: (2, 26, 10)\n",
            "mask: (2, 26)\n"
          ]
        }
      ],
      "source": [
        "num_classes = 10\n",
        "model_cfg = ModelConfig(input_dim=num_classes, output_dim=num_classes, hidden_dim=32)\n",
        "model = ElmanRNN(model_cfg, nonlinearity=\"relu\")\n",
        "# model = LSTM(model_cfg)\n",
        "# model = LinearRecurrentUnit(model_cfg)\n",
        "# model = UnitaryRNN(model_cfg)\n",
        "params = model.initialize(jax.random.PRNGKey(0))\n",
        "\n",
        "dataset = CopyDataset(min_lag=10, max_lag=10, batch_size=2, num_classes=num_classes, seq_length=8)\n",
        "inputs, targets, mask = dataset()\n",
        "inputs_oh = jax.nn.one_hot(inputs, num_classes, dtype=jnp.float32)\n",
        "print(\"inputs one-hot:\", inputs_oh.shape)\n",
        "print(\"mask:\", mask.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "outputs, runtime_tensors = model.apply(params, inputs_oh, mask, return_features=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "hidden_to_hidden_weight = params[\"wh\"][\"w\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "nonlin_jacobian_diag = runtime_tensors.nonlinearity_jacobian_diag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "epsilon_values = (0.1, 0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "lookback_norms = _jacobian_lookback_frobenius(nonlin_jacobian_diag, hidden_to_hidden_weight, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "l_eff = _compute_l_eff(lookback_norms, epsilon_values, mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Array([[ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [ 0.,  0.],\n",
              "       [11.,  6.],\n",
              "       [11.,  6.],\n",
              "       [11.,  6.],\n",
              "       [10.,  6.],\n",
              "       [10.,  7.],\n",
              "       [10.,  6.],\n",
              "       [10.,  6.],\n",
              "       [11.,  6.]], dtype=float32)"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "l_eff[0]"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
